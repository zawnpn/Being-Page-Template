<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Being-H0</title>
    
    <!-- Favicon -->
    <link rel="icon" type="image/x-icon" href="assets/image/icon/favicon.ico">
    
    
    <!-- Preload critical assets -->
    <link rel="preload" as="image" href="assets/image/being-h0.png">
    <link rel="preload" as="video" href="assets/image/background.mp4">
    
    <!-- External CSS -->
    <link rel="stylesheet" href="static/css/styles.css">
    
</head>
<body>
    <!-- Theme toggle removed - light theme only -->

    <!-- Header Section -->
    <header class="header" id="header">
        <!-- Background Video -->
        <video class="background-video" autoplay muted loop playsinline preload="auto" poster="assets/image/background.png">
            <source src="assets/image/background.mp4" type="video/mp4">
            <!-- Fallback for browsers that don't support video -->
            <div class="video-fallback"></div>
        </video>
        
        <!-- Dark overlay to make video background less bright -->
        <div class="video-overlay"></div>
        
        <div class="header-content">
            <div class="logo">
                <img src="assets/image/being-h0.png" alt="Project Logo" loading="eager">
            </div>

            <h1 class="title">Scaling Vision-Language-Action Models for Dexterous Hands from Human Videos</h1>

            <!-- Authors with clickable links -->
            <div class="authors">
                <a href="#" class="author-link">Author 1</a><sup>1*</sup>, &nbsp;&nbsp;
                <a href="#" class="author-link">Author 2</a><sup>1*</sup>, &nbsp;&nbsp;
                <a href="#" class="author-link">Author 3</a><sup>2</sup>, &nbsp;&nbsp;
                <a href="#" class="author-link">Author 4</a><sup>2</sup>, &nbsp;&nbsp;
                <a href="#" class="author-link">Author 5</a><sup>1,2â€ </sup>
            </div>
            
            <div class="institutions">
                <sup>1</sup>Peking University &nbsp;&nbsp; <sup>2</sup>BeingBeyond
            </div>

            <div class="contribution">
                <i>
                <sup>*</sup>Equal Contribution &nbsp;&nbsp; <sup>â€ </sup>Corresponding Author
                </i>
            </div>

            <!-- Publication Info -->
            <!-- <div class="publication-info">ICCV 2025</div> -->
            
            <!-- Action Buttons -->
            <div class="action-buttons">
                <a href="#" class="action-btn">
                    <img src="assets/image/icon/pdf.svg" alt="PDF" class="btn-icon">
                    Paper
                </a>
                <a href="#" class="action-btn">
                    <img src="assets/image/icon/arxiv.svg" alt="arXiv" class="btn-icon">
                    arXiv
                </a>
                <a href="https://github.com/BeingBeyond/Being-H0" target="_blank" class="action-btn">
                    <img src="assets/image/icon/github.svg" alt="GitHub" class="btn-icon">
                    Code
                </a>

                <a href="https://huggingface.co/BeingBeyond/Being-H0" target="_blank" class="action-btn">
                    <img src="assets/image/icon/hf.svg" alt="Model" class="btn-icon">
                    Model
                </a>
            </div>
        </div>
    </header>

    <div class="container">
        <!-- Abstract Section -->
        <section class="section" id="abstract">
            <h2 class="section-title">Abstract</h2>
            <div class="abstract">
                <p class="abstract-text">
                    Multimodal large language models (MLLMs) have made significant progress in vision-language understanding, yet effectively aligning different modalities remains a fundamental challenge. We present a framework that unifies multimodal understanding by applying byte-pair encoding to visual tokens. Unlike conventional approaches that rely on modality-specific encoders, our method directly incorporates structural information into visual tokens, mirroring successful tokenization strategies in text-only language models. We introduce a priority-guided encoding scheme that considers both frequency and spatial consistency, coupled with a multi-stage training procedure based on curriculum-driven data composition. These enhancements enable the transformer model to better capture cross-modal relationships and reason with visual information. Comprehensive experiments demonstrate improved performance across diverse vision-language tasks. By bridging the gap between visual and textual representations, our approach contributes to the advancement of more capable and efficient multimodal foundation models.
                </p>
            </div>
        </section>


        <!-- Interactive Demo Section -->
        <section class="section" id="interactive-demo">
            <h2 class="section-title">Interactive Demo</h2>
            
            <!-- Demo Selection with Two-Level Sliders -->
            <div class="nav-section demo-section">
                
                <!-- Dataset Selection (First Row) -->
                <div class="dataset-section">
                    <h3 class="demo-section-title"><b>1. Select Scenario</b></h3>
                    <div class="slider-container">
                        <button class="slider-btn slider-btn-left" id="datasetSliderLeft" onclick="slideDatasets('left')" style="display: none;">â€¹</button>
                        <div class="slider-viewport">
                            <div class="slider-track" id="datasetTrack">
                                <!-- Dataset cards will be populated by JavaScript -->
                            </div>
                        </div>
                        <button class="slider-btn slider-btn-right" id="datasetSliderRight" onclick="slideDatasets('right')" style="display: none;">â€º</button>
                    </div>
                </div>
                
                <!-- Task Selection (Second Row) -->
                <div class="task-section">
                    <h3 class="demo-section-title"><b>2. Select Task</b></h3>
                    <div class="slider-container">
                        <button class="slider-btn slider-btn-left" id="taskSliderLeft" onclick="slideTasks('left')" style="display: none;">â€¹</button>
                        <div class="slider-viewport">
                            <div class="slider-track" id="taskTrack">
                                <!-- Task cards will be populated by JavaScript -->
                            </div>
                        </div>
                        <button class="slider-btn slider-btn-right" id="taskSliderRight" onclick="slideTasks('right')" style="display: none;">â€º</button>
                    </div>
                </div>
            </div>

            <!-- Fixed Video Display Area -->
            <div class="demo-video-container">
                <!-- Reset Button -->
                <button class="demo-reset-btn" onclick="resetDemo()">Reset</button>
                
                <!-- Integrated Breadcrumb Navigation -->
                <div class="breadcrumb-nav" id="breadcrumbNav">
                    <div class="breadcrumb-content">
                        <div class="breadcrumb-path" id="breadcrumbPath">
                            <!-- Current path will be populated by JavaScript -->
                        </div>
                    </div>
                </div>
                
                <div class="video-display" id="videoDisplay">
                    <div class="demo-placeholder">
                        <div class="icon">ðŸŽ¬</div>
                        <p>Select a scenario to begin</p>
                    </div>
                </div>
            </div>
        </section>

        <!-- Video Demonstrations -->
        <section class="section" id="demos">
            <h2 class="section-title">Video Demo</h2>
            <div class="video-grid">
                <div class="video-item">
                    <video loop muted playsinline loading="lazy" preload="none" data-src="assets/video/real_demo/unfold_clothes.mp4">
                        <source data-src="assets/video/real_demo/unfold_clothes.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                    <div class="video-title">Unfold Clothes</div>
                    <div class="video-description">Showcasing multi-finger coordination to manipulate a deformable object.</div>
                </div>

                <div class="video-item">
                    <video loop muted playsinline loading="lazy" preload="none" data-src="assets/video/real_demo/close_lid.mp4">
                        <source data-src="assets/video/real_demo/close_lid.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                    <div class="video-title">Close Lid</div>
                    <div class="video-description">Showcasing high precision and stable control by securely closing a cup lid.</div>
                </div>

                <div class="video-item">
                    <video loop muted playsinline loading="lazy" preload="none" data-src="assets/video/real_demo/close_toolbox.mp4">
                        <source data-src="assets/video/real_demo/close_toolbox.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                    <div class="video-title">Close Toolbox</div>
                    <div class="video-description">Demonstrating interaction with articulated objects by smoothly closing a hinged toolbox.</div>
                </div>

                <div class="video-item">
                    <video loop muted playsinline loading="lazy" preload="none" data-src="assets/video/real_demo/pick_place_1.mp4">
                        <source data-src="assets/video/real_demo/pick_place_1.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                    <div class="video-title">Pick and Place</div>
                    <div class="video-description">Executing natural language commands to test language-grounded manipulation.</div>
                </div>

                <div class="video-item">
                    <video loop muted playsinline loading="lazy" preload="none" data-src="assets/video/real_demo/pick_place_2.mp4">
                        <source data-src="assets/video/real_demo/pick_place_2.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                    <div class="video-title">Pick and Place (clutter)</div>
                    <div class="video-description">Locating and grasping a target in a cluttered scene, highlighting robust perception.</div>
                </div>

                <div class="video-item">
                    <video loop muted playsinline loading="lazy" preload="none" data-src="assets/video/real_demo/pour_cup.mp4">
                        <source data-src="assets/video/real_demo/pour_cup.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                    <div class="video-title">Pour Cup</div>
                    <div class="video-description">Highlighting smooth and precise control by pouring items from one container to another.</div>
                </div>

            </div>
        </section>

        <!-- One Column Layout Example -->
        <section class="section" id="single-column">
            <h2 class="section-title">System Overview</h2>
            <div class="grid-one-column">
                <div class="grid-item">
                    <div class="item-image">
                        <img data-src="assets/image/overview.png" alt="System overview" style="width: 60%; height: 60%; object-fit: cover;" loading="lazy" class="lazy-image">
                    </div>
                    <h3 class="item-title">Unified Architecture Design</h3>
                    <p class="item-description"><b>Being-H0</b> learns dexterous manipulation from large-scale human demonstrations in the UniHand dataset through \texttt{physical instruction tuning}. The resulting foundation model naturally enables effective transfer from human hand demonstrations to robotic dexterous manipulation.</p>
                </div>
            </div>
        </section>

        <!-- Two Column Layout Example -->
        <section class="section" id="two-column">
            <h2 class="section-title">Key Components</h2>
            <div class="grid-two-column">
                <div class="grid-item">
                    <div class="item-image">
                        <img data-src="assets/image/framework.png" alt="VLM architecture" style="width: 100%; height: 100%; object-fit: cover;" loading="lazy" class="lazy-image">
                    </div>
                    <h3 class="item-title">Vision-Language Integration</h3>
                    <p class="item-description">Advanced multimodal understanding capability that processes visual scenes and natural language instructions simultaneously, enabling intuitive human-robot communication and complex task planning.</p>
                </div>

                <div class="grid-item">
                    <div class="item-image">
                        <img data-src="assets/image/framework.png" alt="Modular skills" style="width: 100%; height: 100%; object-fit: cover;" loading="lazy" class="lazy-image">
                    </div>
                    <h3 class="item-title">Modular Skills System</h3>
                    <p class="item-description">A flexible skill composition framework that allows dynamic combination of primitive actions into complex behaviors, enabling adaptable and efficient task execution across diverse scenarios.</p>
                </div>
            </div>
        </section>

        <!-- Three Column Layout Example -->
        <section class="section" id="three-column">
            <h2 class="section-title">Technical Details</h2>
            <div class="grid-three-column">
                <div class="grid-item">
                    <div class="item-image">
                        <img data-src="assets/image/framework.png" alt="Training pipeline" style="width: 100%; height: 100%; object-fit: cover;" loading="lazy" class="lazy-image">
                    </div>
                    <h3 class="item-title">Training Strategy</h3>
                    <p class="item-description">Multi-stage training protocol combining supervised learning, reinforcement learning, and self-supervised techniques for robust skill acquisition.</p>
                </div>

                <div class="grid-item">
                    <div class="item-image">
                        <img data-src="assets/image/framework.png" alt="Performance results" style="width: 100%; height: 100%; object-fit: cover;" loading="lazy" class="lazy-image">
                    </div>
                    <h3 class="item-title">Benchmark Results</h3>
                    <p class="item-description">State-of-the-art performance across multiple robotic benchmarks, demonstrating superior task completion rates and interaction quality.</p>
                </div>

                <div class="grid-item">
                    <div class="item-image">
                        <img data-src="assets/image/framework.png" alt="Applications" style="width: 100%; height: 100%; object-fit: cover;" loading="lazy" class="lazy-image">
                    </div>
                    <h3 class="item-title">Applications</h3>
                    <p class="item-description">Diverse real-world applications spanning household assistance, manufacturing, and service robotics with proven effectiveness.</p>
                </div>
            </div>
        </section>

        <!-- BibTeX Citation Section -->
        <section class="section" id="citation">
            <h2 class="section-title">Citation</h2>
            
            <!-- First BibTeX Entry -->
            <div class="bibtex-container">
                <div class="bibtex-header">
                    <h3>Being-VL-0.5</h3>
                    <button class="copy-btn" onclick="copyBibTeX('bibtex-content-1')">Copy</button>
                </div>
                <pre class="bibtex-code" id="bibtex-content-1">@inproceedings{zhang2025beingvl05,
  title={Unified Multimodal Understanding via Byte-Pair Visual Encoding},
  author={Zhang, Wanpeng and Feng, Yicheng and Luo, Hao and Li, Yijiang and Yue, Zihao and Zheng, Sipeng and Lu, Zongqing},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  year={2025}
}</pre>
            </div>
        </section>
    </div>

    <!-- Footer -->
    <footer class="footer">
        <div class="footer-content">
            <p>This website is made by <a href="https://github.com/BeingBeyond" target="_blank">BeingBeyond</a>. To use our template, you should follow the <a href="https://creativecommons.org/licenses/by-sa/4.0/" target="_blank">CC BY-SA 4.0</a> license.</p>
        </div>
    </footer>

    <!-- External JavaScript -->
    <script src="static/js/script.js" defer></script>
    
</body>
</html>