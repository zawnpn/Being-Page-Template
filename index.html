<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Being-H0</title>
    
    <!-- Favicon -->
    <link rel="icon" type="image/x-icon" href="assets/image/icon/favicon.ico">
    
    
    <!-- Preload critical assets -->
    <link rel="preload" as="image" href="assets/image/being-h0.png">
    <link rel="preload" href="assets/video/background.webm" as="video" type="video/webm">
    
    <!-- External CSS -->
    <link rel="stylesheet" href="static/css/styles.css">
    
</head>
<body>
    <!-- Theme toggle removed - light theme only -->

    <!-- Header Section -->
    <header class="header" id="header">
        <!-- Background Video -->
        <video class="background-video" autoplay muted loop playsinline preload="metadata" disablepictureinpicture controlslist="nodownload nofullscreen noremoteplayback">
            <source src="assets/video/background.webm" type="video/webm">
            <!-- Fallback for browsers that don't support video -->
            <div class="video-fallback"></div>
        </video>
        
        <!-- Dark overlay to make video background less bright -->
        <div class="video-overlay"></div>
        
        <div class="header-content">
            <div class="logo">
                <img src="assets/image/being-h0.png" alt="Project Logo" loading="eager">
            </div>

            <h1 class="title">Vision-Language-Action Pretraining from Large-Scale Human Videos</h1>

            <!-- Authors with clickable links -->
            <div class="authors">
                <a href="https://scholar.google.com/citations?user=TwuNaTYAAAAJ" target="_blank" class="author-link">Hao Luo</a><sup>1,3<b>*</b></sup>, &nbsp;&nbsp;
                <a href="https://takenpeanut.github.io" target="_blank" class="author-link">Yicheng Feng</a><sup>1,3<b></b>*</b></sup>, &nbsp;&nbsp;
                <a href="https://zhangwp.com" target="_blank" class="author-link">Wanpeng Zhang</a><sup>1,3<b>*</b></sup>, &nbsp;&nbsp;
                <a href="https://zhengsipeng.github.io" target="_blank" class="author-link">Sipeng Zheng</a><sup>3<b>*</b></sup>, &nbsp;&nbsp;
                <br>
                <a href="https://scholar.google.com/citations?user=RTuvoywAAAAJ" target="_blank" class="author-link">Ye Wang</a><sup>2,3</sup>, &nbsp;&nbsp;
                <a href="https://yhqpkueecs.github.io" target="_blank" class="author-link">Haoqi Yuan</a><sup>1,3</sup>, &nbsp;&nbsp;
                <a href="#" class="author-link">Jiazheng Liu</a><sup>1</sup>, &nbsp;&nbsp;
                <a href="https://co1one.github.io" target="_blank" class="author-link">Chaoyi Xu</a><sup>3</sup>, &nbsp;&nbsp;
                <a href="https://www.jin-qin.com" target="_blank" class="author-link">Qin Jin</a><sup>2</sup>, &nbsp;&nbsp;
                <a href="https://z0ngqing.github.io" target="_blank" class="author-link">Zongqing Lu</a><sup>1,3<b>â€ </b></sup>
            </div>
            
            <div class="institutions">
                <sup>1</sup>Peking University &nbsp;&nbsp; <sup>2</sup>Renmin University of China &nbsp;&nbsp; <sup>3</sup>BeingBeyond
            </div>

            <div class="contribution">
                <i>
                <sup><b>*</b></sup>Equal Contribution &nbsp;&nbsp; <sup><b>â€ </b></sup>Corresponding Author
                </i>
            </div>

            <!-- Publication Info -->
            <!-- <div class="publication-info">XXXX 20xx</div> -->
            
            <!-- Action Buttons -->
            <div class="action-buttons">
                <a href="#" class="action-btn">
                    <img src="assets/image/icon/pdf.svg" alt="PDF" class="btn-icon">
                    Paper
                </a>
                <a href="#" class="action-btn">
                    <img src="assets/image/icon/arxiv.svg" alt="arXiv" class="btn-icon">
                    arXiv
                </a>
                <a href="https://github.com/BeingBeyond/Being-H0" target="_blank" class="action-btn">
                    <img src="assets/image/icon/github.svg" alt="GitHub" class="btn-icon">
                    Code
                </a>

                <a href="https://huggingface.co/BeingBeyond/Being-H0" target="_blank" class="action-btn">
                    <img src="assets/image/icon/hf.svg" alt="Model" class="btn-icon">
                    Model
                </a>
            </div>
        </div>
    </header>

    <div class="container">
        <!-- Simple Description -->
        <section class="section" id="description">
            <div class="simple-description">
                We introduce <b><tt>Being-H0</tt></b>, the first dexterous Vision-Language-Action model pretrained from large-scale human videos via explicit hand motion modeling. 
            </div>
        </section>

        <section class="section" id="single-column">
            <div class="grid-one-column">
                <div class="grid-item">
                    <div class="item-image">
                        <img data-src="assets/image/overview.jpg" alt="Key Concept" style="width: 100%; height: 100%; object-fit: cover;" loading="lazy" class="lazy-image">
                    </div>
                    <h3 class="item-title">Key Concept</h3>
                    <p class="item-description"><b><tt>Being-H0</tt></b> acquires dexterous manipulation skills by learning from large-scale human videos in the <b>UniHand</b> dataset via <b>physical instruction tuning</b>. By explicitly modeling hand motions, the resulting foundation model seamlessly transfers from human hand demonstrations to robotic manipulation.</p>
                </div>
            </div>
        </section>

        <!-- Interactive Demo Section -->
        <section class="section" id="interactive-demo">
            <h2 class="section-title">Interactive Demo</h2>
            
            <!-- Demo Selection with Two-Level Sliders -->
            <div class="nav-section demo-section">
                
                <!-- Dataset Selection (First Row) -->
                <div class="dataset-section">
                    <h3 class="demo-section-title"><b>1. Select Scenario</b></h3>
                    <div class="slider-container">
                        <button class="slider-btn slider-btn-left" id="datasetSliderLeft" onclick="slideDatasets('left')" style="display: none;">â€¹</button>
                        <div class="slider-viewport">
                            <div class="slider-track" id="datasetTrack">
                                <!-- Dataset cards will be populated by JavaScript -->
                            </div>
                        </div>
                        <button class="slider-btn slider-btn-right" id="datasetSliderRight" onclick="slideDatasets('right')" style="display: none;">â€º</button>
                    </div>
                </div>
                
                <!-- Task Selection (Second Row) -->
                <div class="task-section">
                    <h3 class="demo-section-title"><b>2. Select Task</b></h3>
                    <div class="slider-container">
                        <button class="slider-btn slider-btn-left" id="taskSliderLeft" onclick="slideTasks('left')" style="display: none;">â€¹</button>
                        <div class="slider-viewport">
                            <div class="slider-track" id="taskTrack">
                                <!-- Task cards will be populated by JavaScript -->
                            </div>
                        </div>
                        <button class="slider-btn slider-btn-right" id="taskSliderRight" onclick="slideTasks('right')" style="display: none;">â€º</button>
                    </div>
                </div>
            </div>

            <!-- Fixed Video Display Area -->
            <div class="demo-video-container">
                <!-- Reset Button -->
                <button class="demo-reset-btn" onclick="resetDemo()">Reset</button>
                
                <!-- Integrated Breadcrumb Navigation -->
                <div class="breadcrumb-nav" id="breadcrumbNav">
                    <div class="breadcrumb-content">
                        <div class="breadcrumb-path" id="breadcrumbPath">
                            <!-- Current path will be populated by JavaScript -->
                        </div>
                    </div>
                </div>
                
                <div class="video-display" id="videoDisplay">
                    <div class="demo-placeholder">
                        <div class="icon">ðŸŽ¬</div>
                        <p>Select a scenario to begin</p>
                    </div>
                </div>
            </div>
        </section>

        <!-- Video Demonstrations -->
        <section class="section" id="demos">
            <h2 class="section-title">Robot Demo (1x)</h2>
            <div class="video-grid">
                <div class="video-item">
                    <video loop muted playsinline loading="lazy" preload="none" data-src="assets/video/real_demo/unfold_clothes.mp4">
                        <source data-src="assets/video/real_demo/unfold_clothes.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                    <div class="video-title">Unfold Clothes</div>
                    <div class="video-description">Showcasing multi-finger coordination to manipulate a deformable object.</div>
                </div>

                <div class="video-item">
                    <video loop muted playsinline loading="lazy" preload="none" data-src="assets/video/real_demo/close_lid.mp4">
                        <source data-src="assets/video/real_demo/close_lid.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                    <div class="video-title">Close Lid</div>
                    <div class="video-description">Showcasing high precision and stable control by securely closing a cup lid.</div>
                </div>

                <div class="video-item">
                    <video loop muted playsinline loading="lazy" preload="none" data-src="assets/video/real_demo/close_toolbox.mp4">
                        <source data-src="assets/video/real_demo/close_toolbox.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                    <div class="video-title">Close Toolbox</div>
                    <div class="video-description">Demonstrating interaction with articulated objects by smoothly closing a hinged toolbox.</div>
                </div>

                <div class="video-item">
                    <video loop muted playsinline loading="lazy" preload="none" data-src="assets/video/real_demo/pick_place_1.mp4">
                        <source data-src="assets/video/real_demo/pick_place_1.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                    <div class="video-title">Pick and Place</div>
                    <div class="video-description">Executing natural language commands to test language-grounded manipulation.</div>
                </div>

                <div class="video-item">
                    <video loop muted playsinline loading="lazy" preload="none" data-src="assets/video/real_demo/pick_place_2.mp4">
                        <source data-src="assets/video/real_demo/pick_place_2.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                    <div class="video-title">Pick and Place (clutter)</div>
                    <div class="video-description">Locating and grasping a target in a cluttered scene, highlighting robust perception.</div>
                </div>

                <div class="video-item">
                    <video loop muted playsinline loading="lazy" preload="none" data-src="assets/video/real_demo/pour_cup.mp4">
                        <source data-src="assets/video/real_demo/pour_cup.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                    <div class="video-title">Pour Cup</div>
                    <div class="video-description">Highlighting smooth and precise control by pouring items from one container to another.</div>
                </div>

            </div>
        </section>

        <section class="section" id="single-column">
        <h2 class="section-title">Overview</h2>
            <div class="grid-one-column">
                <div class="grid-item">
                    <div class="item-image">
                        <img data-src="assets/image/01_arch.png" alt="Overview of Being-H0" style="width: 100%; height: 100%; object-fit: cover;" loading="lazy" class="lazy-image">
                    </div>
                    <h3 class="item-title">Overview of <b><tt>Being-H0</tt></b></h3>
                    <p class="item-description">The <b>text tokenizer</b> and <b>visual encoder</b> are shared by both pretraining and post-training. For pretraining and hand motion/translation tasks, <b><tt>Being-H0</tt></b> generates outputs in an autoregressive manner. For post-training and downstream manipulation tasks, <b><tt>Being-H0</tt></b> incorporates a set of learnable queries as the action chunk for prediction.</p>
                </div>
            </div>
        </section>

        <section class="section" id="single-column">
            <div class="grid-one-column">
                <div class="grid-item">
                    <div class="item-image">
                        <img data-src="assets/image/02_phy_inst_tune.png" alt="Physical Instruction Tuning" style="width: 100%; height: 100%; object-fit: cover;" loading="lazy" class="lazy-image">
                    </div>
                    <h3 class="item-title">Physical Instruction Tuning</h3>
                    <p class="item-description"><b>Physical Instruction Tuning</b> unifies human video datasets and robotic manipulation data. We extend VLM to include motion and action tokens, using multi-head attention across vision, text, motion, and action modalities to create a unified VLA model for robotic tasks.</p>
                </div>
            </div>
        </section>

        <section class="section" id="single-column">
            <div class="grid-one-column">
                <div class="grid-item">
                    <div class="item-image">
                        <img data-src="assets/image/04_unihand.png" alt="UniHand Dataset" style="width: 100%; height: 100%; object-fit: cover;" loading="lazy" class="lazy-image">
                    </div>
                    <h3 class="item-title"><tt>UniHand-2.5M</tt></h3>
                    <p class="item-description">The overview of our <b><tt>UniHand-2.5M</tt></b>. Left: The scenes and tasks from different data source types. Mid: The distribution of different data sources, data types, and durations. Right: Samples from different data types.</p>
                </div>
            </div>
        </section>

        <section class="section" id="two-column">
            <h2 class="section-title">Experiments</h2>
            <div class="grid-two-column">
                <div class="grid-item">
                    <div class="item-image">
                        <img data-src="assets/image/08_data_scaling_law.png" alt="Scaling Law" style="width: 72%; height: 72%; object-fit: cover;" loading="lazy" class="lazy-image">
                    </div>
                    <h3 class="item-title">Scaling Performance</h3>
                    <p class="item-description">The performance of <b><tt>Being-H0</tt></b> alongside the increasing training data scale for visual-grounded hand motion generation. It scales well with model and data sizes.</p>
                </div>

                <div class="grid-item">
                    <div class="item-image">
                        <img data-src="assets/image/robot_exp.png" alt="Robot Experiments" style="width: 100%; height: 100%; object-fit: cover;" loading="lazy" class="lazy-image">
                    </div>
                    <h3 class="item-title">Robot Experiments</h3>
                    <p class="item-description">The advantages of <b><tt>Being-H0</tt></b> are particularly remarkable in dexterous manipulation tasks, where it substantially improves the success rate and requires much less teleoperated demonstrations.</p>
                </div>
            </div>
        </section>

        <!-- <section class="section" id="three-column">
            <h2 class="section-title">Technical Details</h2>
            <div class="grid-three-column">
                <div class="grid-item">
                    <div class="item-image">
                        <img data-src="assets/image/framework.png" alt="Training pipeline" style="width: 100%; height: 100%; object-fit: cover;" loading="lazy" class="lazy-image">
                    </div>
                    <h3 class="item-title">Training Strategy</h3>
                    <p class="item-description">Multi-stage training protocol combining supervised learning, reinforcement learning, and self-supervised techniques for robust skill acquisition.</p>
                </div>

                <div class="grid-item">
                    <div class="item-image">
                        <img data-src="assets/image/framework.png" alt="Performance results" style="width: 100%; height: 100%; object-fit: cover;" loading="lazy" class="lazy-image">
                    </div>
                    <h3 class="item-title">Benchmark Results</h3>
                    <p class="item-description">State-of-the-art performance across multiple robotic benchmarks, demonstrating superior task completion rates and interaction quality.</p>
                </div>

                <div class="grid-item">
                    <div class="item-image">
                        <img data-src="assets/image/framework.png" alt="Applications" style="width: 100%; height: 100%; object-fit: cover;" loading="lazy" class="lazy-image">
                    </div>
                    <h3 class="item-title">Applications</h3>
                    <p class="item-description">Diverse real-world applications spanning household assistance, manufacturing, and service robotics with proven effectiveness.</p>
                </div>
            </div>
        </section> -->

        <!-- BibTeX Citation Section -->
        <section class="section" id="citation">
            <h2 class="section-title">Citation</h2>
            
            <!-- First BibTeX Entry -->
            <div class="bibtex-container">
                <div class="bibtex-header">
                    <h3><tt>Being-H0</tt></h3>
                    <button class="copy-btn" onclick="copyBibTeX('bibtex-content-1')">Copy</button>
                </div>
                <pre class="bibtex-code" id="bibtex-content-1">@article{beingbeyond2025beingh0,
  title={Being-H0: Vision-Language-Action Pretraining from Large-Scale Human Videos},
  author={Luo, Hao and Feng, Yicheng and Zhang, Wanpeng and Zheng, Sipeng and Wang, Ye and Yuan, Haoqi and Liu, Jiazheng and Xu, Chaoyi and Jin, Qin and Lu, Zongqing},
  journal={arXiv preprint arXiv:XXXX.XXXXX},
  year={2025}
}</pre>
            </div>
        </section>
    </div>

    <!-- Footer -->
    <footer class="footer">
        <div class="footer-content">
            <p>This website is made by <a href="https://github.com/BeingBeyond" target="_blank">BeingBeyond</a>. To use our template, you should follow the <a href="https://creativecommons.org/licenses/by-sa/4.0/" target="_blank">CC BY-SA 4.0</a> license.</p>
        </div>
    </footer>

    <!-- External JavaScript -->
    <script src="static/js/script.js" defer></script>
    
</body>
</html>